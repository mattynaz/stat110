\documentclass[centertitle, plain]{notes}
% \usepackage{amsmath}
\usepackage{booktabs}       % prettier tables
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\newcommand{\simiid}{\overset{\t{i.i.d}}{\sim}} % ~i.i.d. shorcut
\renewcommand{\P}{\vmathbb P} % probability measure shortcut

% \documentclass{article}
% \usepackage[utf8]{inputenc}
% \usepackage{xcolor}
% \usepackage[colorlinks, linkcolor=black, citecolor=black, urlcolor=red!70]{hyperref}
% \usepackage[top=1.2in, bottom=1.2in, left = 1in, right = 1in]{geometry}
% \usepackage{changepage}     % styling for epigraph
\usepackage{amsmath}        % prettier text in mathmode
% \usepackage{bm}             % math mode boldface
% \usepackage{amsthm}         % theorems, definitions, etc.
% \usepackage{amssymb}        % mathbb, etc.
% \usepackage{float}          % prettier tables
% % \usepackage{thmtools}       % prettier theorems

% \newcommand{\R}{\mathbb R}     % reals shortcut
\renewcommand{\t}{\textnormal} % text shortcut
% \newcommand{\Var}{\t{Var}}     % Var shortcut
% \newcommand{\Cov}{\t{Cov}}     % Cov shortcut
% \newcommand{\Corr}{\t{Corr}}   % Corr shortcut
% \newcommand{\SD}{\t{SD}}       % SD shortcut
% \newcommand{\N}{\mathcal N}    % N shortcut

% % \declaretheorem[style=definition,name=Definition,qed=$\lrcorner$]{definition}
% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}{Proposition}[section]
% \newtheorem{example}{Example}[section]
% \theoremstyle{remark}
% \newtheorem*{note}{Note}

% \setlength{\parindent}{0em}
% \renewcommand{\arraystretch}{1.4}

% \title{Statistics 110: Introduction to Probability}
% \author{Matthew A. Nazari \\ \href{mailto:matthewnazari@college.harvard.edu}{\texttt{matthewnazari@college.harvard.edu}}}
% \date{Summer 2021}



\coursetitle{Introduction to Probability}
\coursecode{stat110}
\subtitle{Textbook Notes}
\professor{Joe Blitzstein}
\scribe{Matthew Nazari\titlehref[mailto:matthewnazari@college.harvard.edu]{matthewnazari@college}[email]}
\season{Summer}
\year{2021}
\flag{%
  \begin{flushleft}``Probability theory is nothing but common sense reduced to calculation''\end{flushleft}
  \hfill {\normalfont-- Pierre-Simon Laplace}%
}


\begin{document}
\newpage

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PROBABILITY AND COUNTING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probability and Counting}

\subsection{Naive Probability and Counting}
\begin{definition}[sample space $\Omega$]
  A sample space $\Omega$ is the set of all possible outcomes to an experiment.
\end{definition}
\begin{definition}[event $A$]
  An event $A \subseteq \Omega$ is a subset of the sample space $\Omega$. We say $A$ occured if the actual outcome of the experiment is in $A$.
\end{definition}
\begin{definition}[naive probability]
  If the sample space of an experiment is finite and all outcomes are equally likely, then the probability an event $A$ is $$P_\t{naive}(A) = \frac{|A|}{|\Omega|}.$$
\end{definition}
\begin{example}
  Consider a randomly shuffed deck of $n$ cards labelled 1 through $n$ where $n$ is an even number. What is the probability of drawing a card labelled with an odd number? Since all $n$ cards can be drawn, the sample space of this experiment is the set of all cards: $$\Omega = \{1, 2, \ldots, n\}.$$ Since the sample space is finite, $|\Omega|=n$, and each card has an equal probability of being drawn\footnote{This was assumed because of symmetry and basic intuition.}, the naive definition of probability applies. The event $A_\t{odd} \subseteq \Omega$ that we are interested in is the set of all outcomes where the card labelled $a$ drawn is odd: $$A_\t{odd}=\{a : a \t{ is odd and } a \leq n\}=\{1, 3, \ldots, n-1\}.$$
  Since the naive definition of probability applies,
  $$P_\t{naive}(A_\t{odd}) = \frac{|A_\t{odd}|}{|\Omega|} = \frac{\frac12 n}{n} = \frac12.$$
\end{example}
\begin{proposition}[multiplication rule]
  Consider an experiment consisting of two sub-experiments $A$ and $B$. If there are $a$ possible outcomes for $A$ and for each of these outcomes there are $b$ possible outcomes for $B$, then the compound experiment has $ab$ outcomes. 
\end{proposition}
\begin{proposition}[sampling with replacement]
  Consider an experiment where there are $n$ objects and $k$ choices made from them with replacement (choosing an object does not preclude it from being chosen again). Then there are $n^k$ outcomes.
\end{proposition}
\begin{proposition}[sampling without replacement]
  Consider an experiment where there are $n$ objects and $k$ choices made from them without replacement (choosing an object precludes it from being chosen again). Then there are $n(n-1)\dots(n-k+1) = \frac{n!}{(n-k)!}$ possible outcomes for $k \leq n$ and 0 possible outcomes otherwise.
\end{proposition}
\begin{definition}[binomial coefficient]
  Consider two nonnegative integers $k$ and $n$. For $k \leq n$, $$\binom{n}{k} = \frac{n(n-1)\dots(n-k+1)}{k!} = \frac{n!}{(n-k)!k!}.$$ For $k > n$, $\binom{n}{k} = 0$. The number $\binom{n}{k}$ is the number of subsets of size $k$ for a set of size $n$.
\end{definition}
\begin{example}
  A hand is 5 cards dealt from a standard, randomly shuffled 52 card deck. In poker, a hand is a full house if any 2 of the cards are of the same rank and the other 3 cards are of another rank. What is the probability of a full house? The naive probability applies since there are $\binom{52}{5}$ possible outcomes and they are all equaly likely. Consider two ways to calculate the number of possible hands that are full houses by applying the multiplication rule (there could be potentially many more ways apply this rule). One way may be to choose one rank, choose 3 cards from this rank, choose one other rank, then choose 2 cards from this rank: $$13\binom{4}{3}12\binom{4}{2}.$$ Another way is to choose 2 ranks, choose one to be the rank of the three cards, choose 3 cards of that rank, then choose 2 cards of the other rank: $$\binom{13}{2}2\binom{4}{3}\binom{4}{2}.$$ Both of these expressions are equal. Therefore, $$P(\t{full house}) = \frac{13\binom{4}{3}12\binom{4}{2}}{\binom{52}{5}} = \frac{\binom{13}{2}2\binom{4}{3}\binom{4}{2}}{\binom{52}{5}} \approx 0.00144.$$
\end{example}
\begin{example}
  In a club of $n$ members there are $n(n-1)(n-2) = \frac{n!}{(n-3)!}$ ways to pick a president, then vice president, then secretary. If we are picking 3 members before assigning positions, then order does not matter. In this case, since we overcounted by a factor of exactly $3!$, there are $\frac{n!}{(n-3)!3!} = \binom{n}{3}$ possible outcomes.
\end{example}

\subsection{General Probability}
\begin{definition}[general probability] \label{def:general probability}
  A probability space $(\Omega, \mathcal F, \P)$ consists of a sample space $\Omega$, a set of events $\mathcal F$, and a probability measure $\P : \mathcal F \to [0, 1]$.\footnote{The probability measure $\P$ is almost always for simplicity written as $P$.} The set $\mathcal F$ is a $\sigma$-algebra of $\Omega$, meaning $\mathcal F$ satisfies the following axioms:
  \begin{itemize}
    \item $\emptyset \in \mathcal F$ and $\Omega \in \mathcal F$.
    \item If $A \in \mathcal F$, then $A^c \in \mathcal F$.
    \item If $A_1, A_2, \ldots \in \mathcal F$, then $\bigcup_{i=1}^{\infty} A_i \in \mathcal F$.
  \end{itemize}
  The probability measure $\P$ also satisfies certain axioms:
  \begin{itemize}
    \item $\P(\emptyset) = 0$ and $\P(\Omega) = 1$.
    \item If $A_1, A_2, \ldots$ are disjoint events, then $\P\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=0}^\infty \P(A_i).$
  \end{itemize}
\end{definition}
\begin{note}
  Unless otherwise stated, we can assume $\mathcal F$ is the set of all subsets of $\Omega$ and denote probability spaces as simply a sample sample space and probability function, $(\Omega, P)$.
\end{note}
\begin{proposition}[basic properties of probability]
  For any event $A$ and $B$, the following properties are true:
  \begin{itemize}
    \item $P(A) = 1 - P(A^c)$.
    \item $P(A \cup B) = P(A) + P(B) - P(A B)$.
    \item If $A \subseteq B$, then $P(A) \leq P(B)$.
    \item $(A \cap B)^c = A^c \cup B^c$ and $(A \cup B)^c = A^c \cap B^c$.\footnote{These properties are commonly known as De Morgan's laws.}
  \end{itemize}
\end{proposition}
\begin{proposition}[inclusion-exclusion principle]
  For any set of events $A_1, \ldots, A_n$, $$P\left(\bigcup_{i=1}^{n} A_{i}\right)=\sum_{i} P\left(A_{i}\right)-\sum_{i<j} P\left(A_{i} \cap A_{j}\right)+ \sum_{i<j<k} P\left(A_{i} \cap A_{j} \cap A_{k}\right)-\ldots+(-1)^{n+1} P\left(A_{1} \cap \cdots \cap A_{n}\right).$$
\end{proposition}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONDITIONAL PROBABILITY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Conditional Probability}

\subsection{Conditioning}
\begin{definition}[conditional probability]
  The conditional probability of an event $A$ given another event $B$, denoted as $P(A|B)$, is $$\frac{P(A \cap B)}{P(B)}.$$ 
\end{definition}
\begin{note}
  Conditional probability answers the question, ``if the outcome to an experiment $\omega$ is in $B$, then what is the probability that $\omega$ is also in $A$?'' In essense, we update the probability that $A$ occurs given our new information that $B$ occurs.
\end{note}
\begin{definition}[prior and posterior probability]
  $P(A)$ is the prior probability of $A$ and $P(A|B)$ is the posterior probability of $A$ after conditioning on $B$.
\end{definition}
\begin{proposition}
  By rearranging the definition of conditional probability, we get $P(A \cap B) = P(A|B)P(B) = P(B|A)P(A)$. Generalizing this, we have\footnote{``A, B'' denotes ``$A \cap B$'' and is read as ``A and B''.} $$P(A_1, \ldots, A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \dots P(A_n|A_1, \ldots, A_{n-1}).$$
\end{proposition}
\begin{proposition}[Bayes' Rule]
  $$P(A|B) = \frac{P(A)P(B|A)}{P(B)}.$$
\end{proposition}
\begin{note}
  It might seem counter intuitive to use $P(B|A)$ to solve for $P(A|B)$. Often times in statistics, however, $P(A|B)$ is easier to find than $P(B|A)$ (or vice versa).
\end{note}
\begin{proposition}[Law of Total Probability, LOTP]
  Let $A_1, \ldots, A_n$ be a partition of the sample space\footnote{$A_1, \ldots, A_n$ are disjoint and $\bigcup_{i=1}^{n} A_i = \Omega$.} $\Omega$ such that $P(A_i) \neq 0$ for all $i$. Then $$P(B) = \sum_{i=1}^{n} P(B|A_i)P(A_i).$$
\end{proposition}
\begin{note}
  How we partition the sample space in order to calculate $P(B)$ is crucial. Some partitions could require us to calculate $n$ difficult probabilities, whereas others might be easier. 
\end{note}
\begin{example}
  There is a fair coin and a coin that lands on heads 3/4 of the time. You conduct an experiment where you pick up a coin and flip it three times. The coin lands on heads 3 times. What is the probability that the fair coin was picked up? Let $A$ be the event that the coin lands on heads 3 times and $F$ be the event the fair coin was picked. Using Bayes' Rule, $$P(F|A) = \frac{P(A|F)P(F)}{P(A)}.$$ By assumption, $P(F) = 1/2$. To calculate $P(A)$ requires LOTP, $$P(A) = P(A|F)P(F) + P(A|F^c)P(F^c) = (1/2)^3(1/2) + (3/4)^3(1/2).$$ Hence, we have $$P(F|A) = \frac{P(A|F)P(F)}{P(A)} = \frac{(1/2)^3(1/2)}{(1/2)^3(1/2) + (3/4)^3(1/2)} \approx 0.23.$$
\end{example}
\begin{example}
  A family has two children. What is the probability that both are girls given the eldest is a girl? Intuitively, we get $$P(\t{both girls $|$ eldest is a girl}) = \frac{P(\t{both girls, eldest is a girl})}{P(\t{eldest is a girl})} = \frac{1/4}{1/2} = 1/2.$$ 
  
  What is the probability that both are girls given at least one of them is a girl? Before, conditioning on the event that the eldest girl is a girl eliminates two outcomes from the sample space $ \Omega= \{BB, BG, GB, GG\}$. However, ``at least one'' does not refer to a specific child and thus only one outcome is knocked out of the sample space. We get $$P(\t{both girls $|$ at least one girl}) = \frac{P(\t{both girls, at least one girl})}{P(\t{at least one girl})} = \frac{1/4}{3/4} = 1/3.$$

  What is the probability that both are girls given at least one of them is a girl who was born in winter? We have $$P(\t{both girls $|$ at least one winter girl}) = \frac{P(\t{both girls, at least one winter girl})}{P(\t{at least one winter girl})}.$$ Notice that $$P(\t{both girls, at least one winter girl}) = P(\t{both girls, at least one winter child}).$$ Since these are independent events, we have $$P(\t{both girls, at least one winter child}) = (1/4)(1-P(\t{both not winter-born})) = (1/4)(1-(3/4)^2)$$ Therefore, $$\frac{P(\t{both girls, at least one winter girl})}{P(\t{at least one winter girl})} = \frac{(1/4)(1-(3/4)^2)}{1-(7/8)^2} = 7/15.$$ Notice that $7/15 \approx 1/2$. By conditioning on the birth season, we narrow down on a specific child is a child since it is unlikely both children have such a specific characteristic.
\end{example}

\subsection{Conditional Probability as Probabilities}
\begin{proposition}
  Given a probability space $(\Omega, P)$, conditioning on $E$ creates a new probability space $(\Omega \cap E, P')$ where $P'$, which we have been denoting as $P(\cdot|E)$, abides by the axioms in Definition \ref{def:general probability}.
\end{proposition}
\begin{note}
   A way to imagine the new probability space $(\Omega', P')$ is to take a subset of the original sample space, namely $\Omega' = \Omega \cap E \subseteq \Omega$, then scaling the probability measure to sum to 1.
\end{note}
\begin{proposition}[Bayes' Rule with conditioning]
  $$P(A|B, E) = \frac{P(A|E)P(B|A, E)}{P(B|E)}.$$
\end{proposition}
\begin{proposition}[LOTP with conditioning]
  Let $A_1, \ldots, A_n$ be a partition of the sample space $\Omega$ such that $P(A_i \cap E) \neq 0$ for all $i$. Then $$P(B|E) = \sum_{i=1}^{n} P(B|A_i, E)P(A_i|E).$$
\end{proposition}
\subsection{Independence}
\begin{definition}[independence of two events] \label{def:independence}
  If two events $A$ and $B$ are independent, then $P(A \cap B) = P(A)P(B)$.
\end{definition}
\begin{note}
  Notice that $P(A \cap B) = P(A)P(B)$ implies $P(A) = P(A|B)$ and $P(B) = P(B|A)$, meaning that knowing $B$ occurs does not update our confidence that $A$ occurs. Also, two disjoint events are usually never independent. If two events $A$ and $B$ are disjoint and indendent, then $P(A \cap B) = P(\emptyset) = 0$. Therefore, disjoint events $A$ and $B$ are only independent if and only if $P(A) = 0$ or $P(B) = 0$.
\end{note}
\begin{proposition}
  If $A$ and $B$ are independent, then $A$ and $B^c$ are independent, $A^c$ and $B$ are independent, and $A^c$ and $B^c$ are independent.
\end{proposition}
\begin{definition}[independence of multiple events]
  If the events in some set $\mathcal S$ are independent, then the events in every subset of $\mathcal S$ are independent.
\end{definition}
\begin{note}
  Pairwise independence is not enough for three events $A$, $B$, $C$ to be independent. Independence is a symmetric relationship but not transitive, meaning if $A$ is independent of $B$ and $B$ is independent of $C$, then $A$ is not necessarily independent of $C$. Take, for example, the case where $P(A), P(B), P(C) \neq 0$ and $A$ and $C$ are disjoint.
\end{note}
\begin{definition}[conditional independence]
  If two events $A$ and $B$ are conditionally independent given $E$, then $P(A \cap B |E) = P(A|E)P(B|E)$.
\end{definition}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RANDOM VARIABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Random Variables}

\subsection{Discrete and Continuous}
\begin{definition}[random variable, r.v.]
  Consider a probability space $(\Omega, P)$. A random variable is a function $X : \Omega \to \R$ that maps outcomes to real numbers. 
\end{definition}
\begin{definition}[cumulative distribution function, CDF]
  The cumulative distribution function of an r.v. $X$ is the function $F_X(x) = P(X \leq x)$.\footnote{When there is no risk for abiguity, $F_X$ is denoted simply by $F$ or some other letter.} The CDF $F_X$ satisfies the following properties:
  \begin{itemize}
    \item Increasing: if $x_1 \leq x_2$, then $F(x_1) \leq F(x_2)$.
    \item Right continuous: for any $a$, $F(a) = \lim_{x \to a^+} F(x)$.
    \item Convergence to 0 and 1: $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to \infty} F(x) = 1$
  \end{itemize}
\end{definition}
\begin{note}[notation warning]
  $P(X > x)$ is defined as $P(\{\omega : X(\omega) \leq x\}) = \sum_{x_i \leq x} p_X(x_i)$, i.e., the probability of the event an outcome occurs which maps to $x$ or lower. Likewise, $P(X = x)$ is defined as $P(\{\omega : X(\omega) = x\})$.
\end{note}
\begin{definition}[discrete r.v.]
  An r.v. $X$ is discrete if there is a finite or countably infinite number of all values that it maps to with a nonzero probability (e.g., multiples of 1/2 in the range $[0, 10]$ and integers greater than $0$).
\end{definition}
\begin{definition}[support of a discrete r.v.]
  The support of a discrete r.v. $X$ is the set $S_X$ of values that $X$ maps to with a nonzero probability: $$S_X = \{x : P(X = x) > 0\}.$$
\end{definition}
\begin{definition}[probability mass function, PMF]
  The probability mass function of a discrete r.v. $X$ is the function $p_X : \R \to [0, 1]$ given by $p_X(x) = P(X = x)$. The PMF $p_X$ of $X$ must satisfy the following criteria: 
  \begin{itemize}
    \item Nonnegative: if $x \in S_X$, then $p_X(x) > 0$. Otherwise, $p_X(x) = 0$.
    \item Sums to 1: $\sum_{x \in S_X}p_X(x) = 1$.
  \end{itemize}
\end{definition}
\begin{definition}[continuous random variable]
  An r.v. $X$ is continuous if its CDF is differentiable with possibly the exception of a few points where the CDF is continuous but not differentiable (e.g., the endpoints).
\end{definition}
\begin{definition}[probability density function, PDF]
  The probability density function of a continuous r.v. $X$ is the derivative $f$ of the CDF $F_X$ of $X$. The PDF $f$ of $X$ must satisfy the following criteria: 
  \begin{itemize}
    \item Nonnegative: $f(x) \geq 0$.
    \item Integrates to 1: $\int_{-\infty}^{\infty} f(x) \,dx= 1$.
  \end{itemize}
\end{definition}
\begin{definition}[support of a continuous r.v.]
  The support of a continuous r.v. $X$ is the set $S_X$ of values that its PDF $f$ maps to with a nonzero probability density: $$S_X = \{x : f(x) > 0\}.$$
\end{definition}
\begin{proposition}[PDF to CDF]
  Let $X$ be a continuous r.v. with PDF $f$. Then the CDF $F_X$ of $X$ is $$F_X(x) = \int_{-\infty}^{x} f(t) \,dt.$$
\end{proposition}
\begin{proposition}[PDF and CDF to probability]
  Let $X$ be a continuous r.v. with PDF $f$ and CDF $F_X$. Then $$P(a \leq x \leq b) = F_X(a) - F_X(b) = \int_{a}^{b} f(t) \,dt.$$
\end{proposition}
\begin{note}
  The probability that a continuous r.v. $X$ takes on a particular value is 0, i.e., $P(X = x) = 0$ for all $x$. The PDF of $X$ can even be greater than 1 at some values.
\end{note}

\subsection{Discrete Distributions}
\begin{definition}[Bernoulli distribution]
  If $X \sim \t{Bern}(p)$,\footnote{The ``$\sim$'' symbol is read as ``is distributed as''.} then $P(X = 1) = p$ and $P(X = 0) = 1 - p$. Any event $A$ has a Bernoulli variable naturally associated with it, where $X$ maps all outcomes in $A$ to 1 and all outcomes in $A^c$ to 0.
\end{definition}
\begin{note}
  There is not one Bernoulli distribution, but a family of Bernoulli distributions indexed by $p$. If $X \sim \t{Bern}(p)$, it is incorrect to say ``$X$ is Bernoulli''. Instead, we say ``$X$ is Bernoulli with parameter $p$'' or ``$X$ has the Bernoulli distribution with parameter $p$''. This applies to all distributions. 
\end{note}
\begin{definition}[indicator random variable]
  If an r.v. equals 1 if an event $A$ occurs and 0 if $A$ does not occur, then it is an indicator random variable of $A$ denoted as $I_A$ or $I(A) \sim \t{Bern}(p)$ where $p = P(A)$.
\end{definition}
\begin{definition}[Bernoulli trial]
  If the only possible outcomes to an experiment are ``success'' and ``failure'', then the experiment is a Bernoulli trial.
\end{definition}
\begin{definition}[binomial distribution]
  If $X \sim \t{Bin}(n, p)$, then $$P(X = k) = \binom{n}{k}p^k(1-p)^{n-k}$$ for $0 \leq k \leq n$ and 0 otherwise. The binomial distribution describes the number successes in $n$ independent Bernoulli trials of success probability $p$.
\end{definition}
\begin{definition}[hypergeometric distribution]
  If $X \sim \t{HGeom}(w, b, n)$, then $$P(X = k) = \frac{\binom{w}{k}\binom{b}{n - k}}{\binom{w + b}{n}}$$ for $0 \leq k \leq w$ and 0 otherwise. The hypergeometric distribution describes the number of white marbles drawn after drawing $n$ marbles from a jar of $w$ white marbles and $b$ black marbles without replacement.
\end{definition}
\begin{definition}[discrete uniform distribution]
  If $X \sim \t{DUnif}(C)$ where $C$ is an finite, nonempty set of numbers, then $$P(X = k) = \frac{1}{|C|}$$ for $k \in C$ and 0 otherwise. The discrete uniform distribution describes the numbers in $C$ as being equally likely. 
\end{definition}
\begin{definition}[geometric distribution]
  If $X \sim \t{Geom}(p)$, then $P(X = k) = q^k p$ for $0 \leq k < \infty$ and 0 otherwise.\footnote{Unless specified otherwise, $q$ always denotes $1 - p$.} The geometric distribution describes the number of failed independent Bernoulli trials until a success.
\end{definition}
\begin{definition}[first success distribution]
  If $X \sim \t{FS}(p)$, then $P(X = k) = q^{k-1} p$ for $1 \leq k < \infty$ and 0 otherwise. The first success distribution describes the number of total independent Bernoulli trials until a success. If $X \sim \t{FS}(p)$, then $X + 1 \sim \t{Geom}(p)$.
\end{definition}
\begin{definition}[negative binomial distribution]
  If $X \sim \t{NBin}(r, p)$, then $$P(X = k) = \binom{k + r - 1}{r - 1} p^r q^k$$ for $0 \leq k < \infty$ and 0 otherwise. The negative binomial distribution describes the number of failed independent Bernoulli trials until the $r$th success.
\end{definition}
\begin{definition}[poisson distribution]
  If $X \sim \t{Pois}(\lambda)$, where $\lambda > 0$, then $$P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}$$ for $0 \leq k < \infty$ and 0 otherwise.
\end{definition}

\subsection{Continuous Distributions}
\begin{definition}[Uniform distribution]
  If $U \sim \t{Unif}(a, b)$, then $U$ has the PDF $f$ and CDF $F$ given by
  \begin{align*}
    f(x) = \begin{cases} \frac{1}{a - b} & \t{if } a < x < b, \\ 0 & \t{otherwise,} \end{cases} &&
    F(x) = \begin{cases} 0 & \t{if } x \leq a, \\ \frac{x - a}{a - b} & \t{if } a < x < b, \\ 1 & \t{if } x \geq b. \end{cases}
  \end{align*}
\end{definition}
\begin{definition}[Logistic distribution]
  If $X \sim \t{Logistic}$, then $X$ has the PDF $f$ and CDF $F$ given by
  \begin{align*}
    f(x) = \frac{e^x}{(1+e^x)^2}, \quad x \in \R, &&
    F(x) = \frac{e^x}{1+e^x}, \quad x \in \R.
  \end{align*}
\end{definition}
\begin{definition}[Rayleigh distribution]
  If $X \sim \t{Rayleigh}$, then $X$ has the PDF $f$ and CDF $F$ given by
  \begin{align*}
    f(x) = xe^{-x^2/2}, \quad x > 0, &&
    F(x) = 1 - e^{-x^2/2}, \quad x > 0.
  \end{align*}
\end{definition}
\begin{definition}[standard Normal distribution]
  If $Z \sim \N(0, 1)$, then $Z$ has the PDF $\varphi$ and CDF $\Phi$ given by
  \begin{align*}
    \varphi(z) = \frac{1}{\sqrt{2\pi}}e^{-z^2/2}, \quad z \in \R, &&
    \Phi(z) = \int_{-\infty}^{z} \varphi(t) \,dt, \quad z \in \R.
  \end{align*}
\end{definition}
\begin{definition}[Normal distribution]
  If $X \sim \N(\mu, \sigma^2)$, then $X = \sigma Z + \mu$ has the PDF $f$ and CDF $F$ given by
  \begin{align*}
    f(x) = \varphi\left(\frac{s - \mu}{\sigma}\right)\frac{1}{\sigma}, \quad x \in \R, &&
    F(x) = \Phi\left(\frac{s - \mu}{\sigma}\right), \quad x \in \R.
  \end{align*}
\end{definition}
\begin{note}
  It is mathematically impossible to find a closed form of $\Phi$, so it is left in integral form.
\end{note}
\begin{definition}[Exponential distribution]
  If $X \sim \t{Expo}(\lambda)$, then $X$ has the PDF $f$ and CDF $F$ given by
  \begin{align*}
    f(x) = \lambda e^{-\lambda x}, \quad x > 0, &&
    F(x) = 1 - e^{-\lambda x}, \quad x > 0.
  \end{align*}
\end{definition}

\subsection{Functions and Independence of Random Variables}
\begin{definition}[function of an r.v.] \label{def:function of an r.v.}
  Consider a probability space $(\Omega, P)$, a discrete r.v. $X$, and a function $g : \R \to \R$. Then $Y = g(X)$ is an r.v. given by $Y = g(X(\omega))$ for all $\omega \in \Omega$. If $g$ is injective, then $P(Y = g(x)) = P(X = x)$. Otherwise, $$P(Y = y) = \sum_{x \t{ s.t. } g(x) = y}P(X = x).$$
\end{definition}
\begin{definition}[function of two r.v.s]
  Consider a probability space $(\Omega, P)$, two discrete r.v.s $X$ and $Y$, and a function $g : \R \times \R \to \R$. Then $Z = g(X, Y)$ is an r.v. given by $Z = g(X(\omega), Y(\omega))$ for all $\omega \in \Omega$. If $g$ is injective, then $P(Z = g(x, y)) = P(X = x, Y = y)$. Otherwise, $$P(Z = z) = \sum_{x, y \t{ s.t. } g(x, y) = z}P(X = x, Y = y).$$
\end{definition}
\begin{proposition}
  If $X \sim \t{Bin}(n, p)$, then $n - X \sim \t{Bin}(n, q)$.
\end{proposition}
\begin{proposition}[sum of binomial r.v.s with same parameter $p$]
  If $X \sim \t{Bin}(n, p)$ and $Y \sim \t{Bin}(m, p)$, then $X + Y \sim \t{Bin}(n + m, p)$.
\end{proposition}
\begin{example}
  A particle moves $n$ steps on a number line starting from position $0$. All steps are independent and equally probable to be either 1 unit left or right.
  \begin{itemize}
    \item Let $X$ be the number of steps to the right after $n$ steps. What is the distribution of $X$? Consider each step as a Bernoulli trial where a success is a move to the right. Since $X$ describes the number of successful steps to the right out of $n$ trials, $X \sim \t{Bin}(n, 1/2)$.
    \item Let $Y$ be the position of the particle after $n$ steps. What is the distribution of $Y$? Notice that since the particle must moves $X$ steps to the right and $n - X$ steps to the left, its final position is $X - (n - X) = 2X - n$. We have expressed $Y$ as an injection of $X$, namely $Y = 2X - n$. We can solve for $$P(Y = k) = P(2X - n = k) = P(X = (k + n)/2) = \binom{n}{\frac{k + n}{2}}\left(\frac 12\right)^n.$$
    \item Let $D$ be the distance of the particle after $n$ steps from the origin. What is the distribution of $D$? Note that $D = |Y|$. By Definition \ref{def:function of an r.v.}, we can solve for the distribution of $D$ despite $D$ not being an injection of $Y$: $$P(D = k) = P(Y = k) + P(Y = - k) = 2\binom{n}{\frac{k + n}{2}}\left(\frac 12\right)^n.$$
  \end{itemize}
\end{example}
\begin{definition}[independence of two r.v.s]
  Two r.v.s $X$ and $Y$ are independent if for all $x$ in the support of $X$ and $y$ in the support of $Y$, $$P(X \leq x, Y \leq y) = P(X \leq x)P(Y \leq y).$$ Intuitively, $X$ and $Y$ are independent of they do not provide any information about the other.
\end{definition}
\begin{definition}[independence of many r.v.s]
  Many r.v.s $X_1, \ldots, X_n$ are independent if for all $x_1, \ldots, x_n$, $$P(X_1 \leq x_1, \ldots, X_n \leq x_n) = P(X_1 \leq x_1) \ldots P(X_n \leq x_n).$$
\end{definition}
\begin{note}
  Independence of $n$ r.v.s implies independence of every subset of these r.v.s. However, this implication is not true backwards: for example, pairwise independence between all $\binom{n}{2}$ pairs of r.v.s is not enough to imply independence of all $n$ r.v.s.
\end{note}
\begin{proposition}
  If $X$ and $Y$ are independent r.v.s, then any function of $X$ is independent of any function of $Y$.
\end{proposition}
\begin{definition}[independent and identically distributed, i.i.d]
  If r.v.s are i.i.d., then they are independent and have the same CDF.
\end{definition}
\begin{note}
  Whether two r.v.s are independent has nothing to do with whether they are identically distributed. 
  \begin{itemize}
    \item Two r.v.s can be independent and identically distributed. Let $X$ be the value of a die roll and $Y$ be the value of a second, independent die roll. $X$ and $Y$ are i.i.d.
    \item Two r.v.s can be independent and not identically distributed. Let $X$ be the value of a die roll and $Y$ be the blackjack value of a card drawn from a standard, well-shuffled deck. Both $X$ and $Y$ provide no information about the other, and they obviously have different distributions.
    \item Two r.v.s can be dependent and identically distributed. Let $X$ be the number of heads in $n$ consecutive, independent coin tosses and $Y$ be the number of tails. Both $X$ and $Y$ are distributed as $\t{Bin}(n, 1/2)$, but they are obviously dependent. 
    \item Two r.v.s can be dependent and not identically distributed. Let $X$ be an indicator variable for if two coin tosses both land on heads and $Y$ be the number of heads that land.
  \end{itemize}
\end{note}
\begin{proposition}[conditioning on a Uniform r.v.]
  Let $U \sim \t{Unif}(a, b)$ and let $(c, d) \subseteq (a, b)$. Then the conditional distribution of $U$ given $U \in (c, d)$ is $\t{Unif}(c, d)$.
\end{proposition}
\begin{definition}[location-scale transformation]
  Let $X$ be an r.v. and $Y = \sigma X + \mu$ where $\sigma ,\, \mu > 0$. Then $Y$ is a location-scale transformation of $X$ where $\mu$ controls how the location is changed and $\sigma$ controls how the scale is changed.
\end{definition}
\begin{proposition}[Universality of the Uniform]
  Let $F$ be a CDF which is continous and strictly increasing on the support of the distribution (i.e., the inverse of the CDF, or quantile function, $F^{-1}: (0, 1) \to \R$ exists). Then the following is true:
  \begin{enumerate}
    \item If $U \sim \t{Unif}(0, 1)$, then $X = F^{-1}(U)$ is an r.v. with the CDF $F$.
    \item If $U$ has the CDF $F$, then $F(X) \sim \t{Unif}(0, 1)$.
  \end{enumerate}
\end{proposition}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXPECTATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Expectation}

\subsection{Expectation}
\begin{definition}[expected value of a discrete r.v.] \label{def:expected value}
  Let $X$ be an discrete r.v. with support $S_X$. Then the expected value of $X$ is $$\E[X] = \sum_{x \in S_X} x P(X = x).$$
\end{definition}
\begin{definition}[expected value of a continuous r.v.] \label{def:expected value continous}
  Let $X$ be an continuous r.v. with PDF $f$. Then the expected value of $X$ is $$\E[X] = \int_{-\infty}^{\infty} x f(x) \,dx.$$
\end{definition}
\begin{note}
  The expected value of a discrete r.v. $X$ is the average of all possible values of $X$ weighted by their probability. Sometimes this sum does not converge, but this is never explained.
\end{note}
\begin{example}
  Let $X$ be the result of rolling a fair 6-sided die. By definition, $$\E[X] = \sum_{x \in S_X} x P(X = x) = \frac16(1) + \frac16(2) + \frac16(3) + \frac16(4) + \frac16(5) + \frac16(6) = 3.5.$$ Notice that $\E[X] \not\in S_X$. This is alright in the same way saying ``the average household in Cambridge has 1.8 children'' is reasonable despite it being impossible for a household to actually have 1.8 children.
\end{example}
\begin{proposition}[expectation of i.d. r.v.s]
  If $X$ and $Y$ are i.d., then $\E[X] = \E[Y]$.
\end{proposition}
\begin{proposition}[linearity of expection]
  If $X$ and $Y$ are r.v.s, independent or not, then $$\E[X + Y] = \E[X] + \E[Y],$$ $$\E[cX] = c\E[X] \t{ for all } c \in \R.$$
\end{proposition}
\begin{proposition}[monotonicity of expectation]
  If $X$ and $Y$ are r.v.s and $X \leq Y$ with a probability of 1, then $\E[X] \leq \E[Y]$.
\end{proposition}
\begin{example}[binomial expectation]
  Let $X \sim \t{Bin}(n, p)$. There are two ways to calculate $\E[X]$. By the definition of expectation, and many algebraic steps, $$\E[X] = \sum_{k = 0}^{n} kP(X = k) = k \binom{n}{k} p^k q^{n-k} = \ldots = np.$$ More simply, recall that $X$ can be expressed as the sum of $n$ independent Bernoulli variables distributed as $\t{Bern}(p)$. By the expectation of i.d. r.v.s and linearity of expectation, $$\E[X] = \E[I_1 + \ldots + I_n] = n\E[I_j] = n(1p + 0q) = np.$$
\end{example}
\begin{example}[hypergeometric expectation]
  Let $X \sim \t{HGeom}(w, b, n)$. Recall that $X = I_1, \ldots, I_n$ where $I_1, ..., I_n$ are dependent r.v.s distributed as $\t{Bern}(w/(w + b))$. These r.v.s are not independent, but nevertheless the linearity of expectation applies: $$\E[X] = \E[I_1 + \ldots + I_n] = n\E[I_j] = nw/(w + b).$$
\end{example}
\begin{example}[geometric expectation]
  Let $X \sim \t{Geom}(p)$. Then $$\E[X] = \sum_{k=0}^\infty kq^kp = pq\sum_{k=0}^\infty kq^{k-1} = pq \frac{1}{(1-q)^2} = \frac{q}{p}.$$
\end{example}
\begin{example}[first success expectation] \label{ex:FS expectation}
  Let $X \sim \t{FS}(p)$. Recall that $X = Y - 1$ where $Y \sim \t{Geom}(p)$. Then $$\E[X] = \E[Y + 1] = \frac{q}{p} + 1 = \frac1p.$$
\end{example}
\begin{example}[negative binomial expectation]
  Let $X \sim \t{NBin}(r, p)$. Recall that $X = X_1 + \ldots + X_r$ where $X_1, ..., X_r$ are independent r.v.s distributed as $\t{Geom}(p)$. Then $$\E[X] = \E[X_1 + \ldots + X_r] = r\E[X_j] = r\frac{q}{p}.$$
\end{example}
\begin{note}
  Note that if $X = Y + 1$, like in Example \ref{ex:FS expectation}, then $\E[X] = \E[Y] + 1$ since $X$ is a linear function $g(x) = x + 1$ of $Y$, so $\E[g(X]) = g(\E[X])$. However, if $g$ is not linear, then $\E[g(X])$ can be very different from $g(\E[X])$. One way to find $\E[g(X])$ is to first find the distirbution of $g(X)$, but $\E[g(X])$ can be calculated directly from the distribution of $X$ using the law of the unconscious statistician.
\end{note}
\begin{definition}[law of the unconscious statistician, LOTUS]
  Let $X$ be a discrete r.v. with support $S_X$ and let $g : \R \to \R$ be a function. Then $$\E[g(X]) = \sum_{x \in S_X}g(x)P(X = x).$$ Let $X$ be a continuous r.v. with PDF $f$ and let $g : \R \to \R$ be a function. Then $$\E[g(X]) = \int_{-\infty}^{\infty} g(x)f(x) \,dx.$$
\end{definition}
\begin{example}[unbiased estimator]
  Let $X \sim \t{Pois}(\lambda)$ where $\lambda$ is unknown but constant. We try to estimate $\theta = e^{-3\lambda}$ with an estimator $g(X)$. An unbiased estimator $g(X)$ is one such that $\E[g(X]) - \theta = 0$ (i.e., $g(X)$ is expected to equal the estimator). We can prove $g(X) = (-2)^X$ is an unbiased estimator since
  \begin{align*}
    \E(-2)^{X}-\theta &= \sum_{k=0}^{\infty}(-2)^{k} P(X = k) - e^{-2 \lambda} \\
                     &= \sum_{k=0}^{\infty}(-2)^{k} \frac{\lambda^{k}}{k !} e^{-\lambda} - e^{-2 \lambda} \\
                     &= e^{-\lambda}e^{-2 \lambda} - e^{-3 \lambda}=0.
  \end{align*}
  However, $g(X) = (-2)^X$ is a silly estimator because $0 < \theta < 1$ (since $\lambda > 0$). A better estimator for $\theta$ is $h(X)$ where $h(X) = 0$ if $g(X) < 0$, $h(X) = 1$ if $g(X) > 1$, and $h(X) = g(X)$ otherwise.
\end{example}

\subsection{The Fundamental Bridge}
\begin{proposition}[indicator r.v. properties]
  Let $A$ and $B$ be any events. Then
  \begin{enumerate}
    \item $(I_A)^k = I_A \t{ for any } k \in \R$,
    \item $I_{A^c} = 1 - I_A$,
    \item $I_{A \cap B} = I_A I_B$,
    \item $I_{A \cup B} = I_A + I_B - I_A I_B$.
  \end{enumerate}
\end{proposition}
\begin{proposition}[the fundamental bridge between probability and expectation]
  For any event $A$, $$P(A) = \E[I_A].$$
\end{proposition}
\begin{example}[Booleâ€™s inequality]
  We already know that $P(\bigcup_{i = 1}^{n} A_i) \leq \sum_{i = 1}^{n} P(A_i)$, but we can prove this with the fundamental bridge. It is evident that $$I\left(\bigcup_{i = 1}^{n} A_i\right) \leq \sum_{i = 1}^{n} I(A_i).$$ By the monotonicity of expectation and the fundamental bridge, $$\E\left[I\left(\bigcup_{i = 1}^{n} A_i\right)\right] \leq \E\left[\sum_{i = 1}^{n} I(A_i)\right],$$ $$P\left(\bigcup_{i = 1}^{n} A_i\right) \leq \sum_{i = 1}^{n} P(A_i).$$
\end{example}
\begin{example}[matching]
  Consider a shuffled deck of $n$ cards labeled 1 through $n$. A card is a match if its position in the deck corresponds to its label. Let $X$ be the number of matches in the deck. What is $\E[X]$? There is no convenient distribution we know for $X$, but we can write $X = I_1 + \ldots + I_n$ where $$I_j = \begin{cases} 1 & \t{if the $j$th card is a match}, \\ 0 & \t{otherwise}.\end{cases}$$ Notice that $I_j$ is just the indicator variable for the event $A_j$ that the $j$th card is a match. By the fundamental bridge, $\E[I_j] = P(A_j) = \frac{1}{n}$. Therefore, $\E[X] = n\E[I_j] = 1$.
\end{example}
\begin{example}[Putnam problem] Consider a random permutation of numbers 1 through $n$. A local maximum is a position whose number is greater than the numbers surrounding it. What is the average number of local maximuma in a random permutation of numbers 1 through $n$? We are interested in $\sum_{j = 1}^{n} I_j$ where $I_j$ is an indicator variable for position $j$ being a local maximum. We know $\E[I_1], \E[I_n] = 1/2$ since an end position is a local maximum if its single neighbor is less than it. By symmetry, this event has a probability of 1/2. Similarly, we know $E_j = 1/3$ for $1 < j < n$ since position $j$ is a local maximum if its two neighbors are less than it. By symmetry, this has a probability of 1/3. Thus, $$\sum_{j = 1}^{n} I_j = 2 \cdot \frac{1}{2} + (n-2) \cdot \frac{1}{3} = \frac{n + 1}{3}.$$
\end{example}
\begin{proposition}[algebraic form of the binomial] \label{prop:algebraic form of Bin}
  If $X \sim \t{Bin}(n, p)$, then we can write $X$ as the sum of $n$ independent r.v.s distributed as $\t{Bern}(p)$: $$X = I_1 + \ldots + I_n.$$
\end{proposition}
\begin{proposition}[algebraic form of the hypergeometric] \label{prop:algebraic form of HGeom}
  If $X \sim \t{HGeom}(w, b, n)$, then we can write $X$ as the sum of $n$ dependent r.v.s distributed as $\t{Bern}(w/(w + b))$ since unconditionally, each $I_j$ has the same probability by symmetry: $$X = I_1 + \ldots + I_n.$$
\end{proposition}
\begin{proposition}[algebraic form of the negative binomial] \label{prop:algebraic form of NBin}
  If $X \sim \t{NBin}(r, p)$, then we can write $X$ as the sum of $r$ independent r.v.s distributed as $\t{Geom}(p)$: $$X = X_1 + \ldots + X_n.$$
\end{proposition}

\subsection{Variance}
\begin{definition}[variance]
  The variance of an r.v. $X$ is $\Var(X) = \E[X-\E X]^2 = \E[X^2] - (\E X)^2$. The variance is the average squared difference between an r.v. $X$ and its expectation.\footnote{With parentheses, $\Var(X) = \E[(X-\E[X])^2]$. This is unweildy, so $\E[X]$ is often simplified to $\E X$.}
\end{definition}
\begin{definition}[standard deviation]
  The standard deviation of an r.v. $X$ is $\SD(X) = \sqrt{\Var(X)}$. The square root is the average difference in absolute value between an r.v. $X$ and its expectation.
\end{definition}
\begin{proposition}[properties of variance]
  Let $X$ and $Y$ be any r.v.s and $c$ be any scalar. Then
  \begin{enumerate}
    \item $\Var(X + c) = \Var(X),$
    \item $\Var(cX) = c^2\Var(X),$
    \item $\Var(X + Y) = \Var(X) + \Var(Y)$ if $X$ and $Y$ are independent,
    \item $\Var(X) > 0$ if $X$ is not constant and $\Var(X) = 0$ if $X$ is constant, i.e. $|S_X| = 1$.
  \end{enumerate}
\end{proposition}
\begin{example}[binomial variance]
  Let $X \sim \t{Bin}(n, p)$. Recall $X = \sum_{j = 1}^{n} I_j$ where $I_1, \ldots, I_n$ are i.i.d $\t{Bern}(p)$. Then $\Var(I_j) = \E[I_j^2] - (EI_j)^2 = p - p^2 = p(1-p)$. Since $I_1, \ldots, I_n$ are independent, $$\Var(X) = \sum_{j = 1}^{n} \Var(I_j) = n\Var(I_j) = np(1-p).$$
\end{example}
\begin{example}
  Let $U \sim \t{Unif}(-1, 1)$. Then $\E[U] = 0$ because of symmetry. Calculating $\Var(U)$ requires us to calculate $\E[U^2]$ first: by LOTUS, $$\E[U^2] = \int_{-1}^{1} u^2 P(U = u) \,du = \frac{1}{2} \int_{-1}^{1} u^2 \,du = \frac{1}{3}.$$ Therefore, $$\Var(U) = \E[U^2] - (EU)^2 = \E[U^2] = \frac{1}{3}.$$ Notice that $U^2$ is not uniform. Let $F(t)$ be the CDF of $U^2$. Then $F(t) = 0$ for $t < -1$ and $F(t) = 1$ for $t > 1$. If $-1 \leq t \leq 1$, $$F(t) = P(U^2 \leq t) = P(-\sqrt{t} \leq U \leq \sqrt{t}) = \sqrt{t}.$$ The last equality is because the probability $U$ is in an interval is proportional to that interval's length.
\end{example}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% JOINT DISTRIBUTIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Joint Distributions}
We usually care about the relationship between multiple r.v.s in the same experiment. Examining the distribution of an r.v. gives a complete story about it individually, but it explains nothing about its relationship to other r.v.s. Are two Bern(1/2) variables, for example, independent? Or are they indicators of complementary events? Examining the joint distribution of these r.v.s answers these questions.

\subsection{Joint, Marginal, and Conditional Distributions}
\begin{definition}[joint CDF]
  The joint CDF of r.v.s $X$ and $Y$ is the function $F_{X, Y} : \R \times \R \to [0, 1]$ given by $$F_{X, Y}(x, y) = P(X \leq x, Y \leq y).$$ 
\end{definition}
Much like in the univariate case, the joint CDF of discrete r.v.s is unweildy because of its jumps and flat regions. We usually, therefore, work with the joint PMF.
\begin{definition}[joint PMF]
  The joint PMF of discrete r.v.s $X$ and $Y$ is the function $p_{X, Y} : \R \times \R \to [0, 1]$ given by $$p_{X,Y} = P(X = x, Y = y)$$ and must satisfy the following criteria: 
  \begin{itemize}
    \item Nonnegative: $P(X = x, Y = y) \geq 0$ for all $x$ and $y$.
    \item Sums to 1: $\sum_x \sum_y P(X = x, Y = y) = 1$.
  \end{itemize}
\end{definition}
The joint CDF and joint PMF of multiple discrete r.v.s is defined analogously. We can get back to the unconditional or marginal distribution of $X$ by adding all possible values of $Y$. This operation is called marginalizing out $Y$.
\begin{definition}[marginal PMF]
  Let $X$ and $Y$ be discrete r.v.s. The marginal PMF of $X$ is $$p_X(x) = P(X = x) = \sum_{y \in S_Y} P(X = x, Y = y).$$
\end{definition}
We can obtain the marginal CDF of $X$ by a limit, but this is unweildy: $$F_X(x) = P(X \leq x) = \lim_{y \to \infty} P(X \leq x, Y \leq y) = \lim_{y \to \infty} F_{X, Y}(x, y).$$
Suppose we observe a value of $X$ and want to update our distribution of $Y$. Instead of marginalizing out $X$, we can obtain a conditional PMF by fixing $y$ and renormalizing.
\begin{definition}[conditional PMF]
  Let $X$ and $Y$ be discrete r.v.s. The conditional PMF $P(Y = y | X = x)$ is given by $$P(Y = y | X = x) = \frac{P(X = x, Y = y)}{P(X = x)}$$ for all $x$ such that $P(X = x) > 0$ and $0$ otherwise.
\end{definition}
We can relate conditional distributions using Bayes' rule. $$P(Y = y | X = x) = \frac{P(X = x | Y = y)P(Y = y)}{P(X = x)}.$$ Using LOTP, we have another way of getting the marginal PMF. $$P(X = x) = \sum_{y \in S_Y} P(X = x | Y = y)P(Y = y).$$
\begin{definition}[independence of r.v.s]
  If r.v.s $X$ and $Y$ are independent then $$F_{X, Y}(x, y) = F_X(x)F_Y(y).$$
\end{definition}
If $X$ and $Y$ are discrete, this definition is equivalent to the conditions that for all $x$ and $y$ such that $P(X = x) > 0$, $$P(X = x, Y = y) = P(X = x)P(Y = y),$$ $$P(Y = y|X = x) = P(Y = y).$$
Independence means that all conditional PMFs are the same as the marginal PMF.
\begin{example} Consider a simple discrete joint distribution of two Bernoulli r.v.s $X$ and $Y$. The joint distribution is fully specified by the four values $P(X = 1, Y = 1)$, $P(X = 1, Y = 0)$, $P(X = 0, Y = 1)$, and $P(X = 0, Y = 0)$ as represented in the table below. To get the marginal probability $P(X = 1)$, we by definition just add $P(X = 1, Y = 0)$ and $P(X = 1, Y = 1)$ to get 25/100. Likewise, we get that the marginal probability $P(Y = 1)$ is 8/100. The marginal distribution of $X$ is Bern(0.25) and the marginal distribution of $Y$ is Bern(0.08). From the joint distribution, we can tell $X$ and $Y$ are not independent since $P(X = 1, Y = 1) = 5/100 \neq 2/100 = P(X = 1)P(Y = 1)$.
  \begin{table}[H]
  \centering
  \begin{tabular}{@{}ccc|c@{}}
  \toprule
                 & $Y = 1$         & $Y = 0$          & \textbf{Total}    \\ \midrule
  $X = 1$        & $\frac{5}{100}$ & $\frac{20}{100}$ & $\frac{25}{100}$  \\
  $X = 0$        & $\frac{3}{100}$ & $\frac{72}{100}$ & $\frac{75}{100}$  \\ \midrule
  \textbf{Total} & $\frac{8}{100}$ & $\frac{92}{100}$ & $\frac{100}{100}$ \\ \bottomrule
  \end{tabular}
  \end{table}
\end{example}
\begin{example}
  Suppose on some day, a chicken lays a random number of eggs. Each egg independently either hatches with a probability $p$ or does not hatch with a probability $q = 1 - p$. Suppose $X$ is the number of eggs that hatch and $Y$ the number that does not hatch, and the total number of eggs $N = X + Y$ is distributed as $\t{Pois}(\lambda)$. What is the joint PMF of $x$ and $Y$? By the law of total probability, $$P(X = i, Y = j) = \sum_{n = 0}^{\infty} P(X = i, Y = j | N = n)P(N = n).$$ Notice, though, that $P(X = i, Y = j | N = n) = 0$ unless $n = i + j$. Thus, $$P(X = i, Y = j) =  P(X = i, Y = j | N = i + j)P(N = i + j).$$ Conditional on $N = i + j$, $X = i$ and $Y = j$ are the same exact event, so $$P(X = i, Y = j)  = P(X = i | N = i + j)P(N = i + j).$$ By the story of the binomial distribution, $X|N = n \sim \t{Bin}(n, p)$ since $X$ describe the number of successful hatches out of $n$ eggs given $N = n$. And since it is given that $N \sim \t{Pois}(\lambda)$, we get
  \begin{align*}
    P(X = i, Y = j) &= P(X = i | N = i + j)P(N = i + j) \\
                    &= \binom{i + j}{i} p^i q^j \cdot \frac{e^{-\lambda}\lambda^{i + j}}{(i + j)!} \\
                    &= \frac{e^{-\lambda p}(\lambda p)^i}{i!} \cdot \frac{e^{-\lambda q}(\lambda q)^j}{j!}.
  \end{align*}
  Since the joint PMF fators into the product of the Pois$(\lambda p)$ PMF as a function of $i$ and the Pois$(\lambda q)$ PMF as a function of $j$, this tells us two elegant facts: (1) $X \sim \t{Pois}(\lambda p)$ and $Y \sim \t{Pois}(\lambda q)$, and (2) $X$ and $Y$ are independent since their joint PMF is the product of their marginal PMFs. It makes sense, albeit counterintuitively, that $X$ and $Y$ are unconditionally independent since $N$ is random. Conditionally, of course, $X$ and $Y$ are very dependent.
\end{example}
\begin{proposition}
  If $X \sim \t{Pois}(\lambda p)$, $Y \sim \t{Pois}(\lambda q)$, and $X$ and $Y$ are independent, then $N = X + Y \sim \t{Pois}(\lambda)$ and $X|N = n \sim \t{Bin}(n, p)$.
\end{proposition}
\begin{proposition}
  If $N \sim \t{Pois}(\lambda)$ and $X|N = n \sim \t{Bin}(n, p)$, then $X \sim \t{Pois}(\lambda p)$, $Y = N - X \sim \t{Pois}(\lambda q)$, and $X$ and $Y$ are independent.
\end{proposition}
% \footnote{The set $S_{X, Y}$ denotes $S_X \times S_Y$, which is the support of the random vector $(X, Y)$ always when $X$ and $Y$ are independent and sometimes with $X$ and $Y$ are dependent.}
We can analogously consider continuous joint distributions. In order for $X$ and $Y$ to have a continuous joint distribution, we require the joint CDF $F_{X, Y}(x, y) = P(X \leq x, Y \leq y)$ be differentiable with respect to $x$ and $y$.
\begin{definition}[joint PDF]
  The joint PDF of continous r.v.s $X$ and $Y$ is the derivative of their joint CDF with respect to $x$ and $y$, $$f_{X, Y}(x, y) = \frac{\partial^2}{\partial x \partial y} F_{X, Y}(x, y),$$ and satisfies the following properties: 
  \begin{itemize}
    \item Nonnegative: $f_{X, Y}(x, y) \geq 0$ for all $x$ and $y$.
    \item Integrates to 1: $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} f_{X, Y}(x, y) \,dx\,dy = 1$.
  \end{itemize}
\end{definition}
Note that $P(X = x, Y = y) = 0$ for any point $(x, y)$. For a general area $A \subseteq \R^2$, we get $$P((X, Y) \in A) = \iint_A f_{X, Y}(x, y) \,dx\,dy.$$
\begin{definition}[marginal PDF]
  Let $X$ and $Y$ be continuous r.v.s. The marginal PDF of $X$ is $$f_X(x) = \int_{-\infty}^{\infty} f_{X, Y}(x, y) \,dy.$$
\end{definition}
The joint CDF and joint PDF of multiple continuous r.v.s is defined analogously. If we have the joint PDF of $X, Y, Z, W$ but want the joint PDF of $X, Y$, we marginalize out $Y$ and $Z$: $$f_{X, Y}(x, w) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X, Y, Z, W}(x, y, z, w) \,dy\,dz.$$
\begin{definition}[conditional PDF]
  Let $X$ and $Y$ be continuous r.v.s. The conditional PDF $f_{Y|X}(y|x)$ is given by $$f_{Y|X}(y|x) = \frac{f_{X, Y}(x, y)}{f_X(x)}$$ for all $x$ such that $f_X(x) > 0$ and $0$ for $f_X(x) = 0$
\end{definition}
Results like Bayes' rule and LOTP work in the continuous case as well: $$f_{X, Y}(x, y) = f_{Y|X}(y|x) f_X(x),$$ $$f_{Y|X}(y|x)=\frac{f_{X|Y}(x|y) f_Y(y)}{f_X(x)},$$ $$f_X(x) = \int_{-\infty}^{\infty} f_{X|Y}(x|y) f_Y(y) \,dy.$$
If $X$ and $Y$ are continuous, the definition of independence between $X$ and $Y$ is equivalent to the conditions that for all $x$ and $y$ such that $P(X = x) > 0$, $$f_{X, Y}(x, y) = f_X(x)f_Y(y),$$ $$f_{Y|X}(y|x) = f_Y(y).$$
\begin{example}[uniform distribution over unit square]
  Let $(X, Y)$ be a random point in the unit square $\{(x, y) : x, y \in [0, 1]\}$, meaning $$f_{X, Y}(x, y) = \begin{cases} 1 & \t{if } x, y \in [0, 1], \\ 0 & \t{otherwise}. \end{cases}$$ Here are a few observations: $X$ and $Y$ are $\t{Unif}(0, 1)$ marginally. We verify this by computing $$f_X(x) = \int_0^1 f_{X, Y}(x, y) \,dy = \int_0^1 1 \,dy = 1$$ and similarly for $f_Y$. Furthermore, $X$ and $Y$ are independent since the joint PDF factors into the product of the marginal PDFs. Geometrically, independence means every horizontal cross section looks like every other horizontal cross section (and same for vertically). 
\end{example}
\begin{example}[uniform distribution over unit circle]
  Let $(X, Y)$ be a random point in the unit circle $\{(x, y) : x^2 + y^2 \leq 1\}$, meaning $$f_{X, Y}(x, y) = \begin{cases} \frac{1}{\pi} & \t{if } x^2 + y^2 \leq 1, \\ 0 & \t{otherwise}. \end{cases}$$ Here are a few observations: $X$ and $Y$ are not $\t{Unif}(0, 1)$ marginally unlike the previous example. We verify this by computing $$f_X(x) = \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} f_{X, Y}(x, y) \,dy = \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \frac{1}{\pi} \,dy = \frac{1}{\pi}(2\sqrt{1-x^2}), \quad -1 \leq x \leq 1$$ and similarly for $f_Y$. The PDF is denser at $0$ than $\pm 1$, which means $X$ and $Y$ are more likely to take on values near 0. We found the bounds of integration by analyzing what range of values $Y$ can take for $X = x$. Furthermore, $X$ and $Y$ are dependent. Logically, observing that $Y = 0.9$ changes the values that $X$ can take on. Realizing that $f_{X, Y}(0.9, 0.9) \neq f_X(0.9)f_Y(0.9)$ (since $(0.9, 0.9) \not\in S_{X, Y}$ while $0.9 \in S_X$ and $0.9 \in S_Y$) is enough to make $X$ and $Y$ dependent.
\end{example}
\begin{example}[Cauchy PDF]
  Let $X$ and $Y$ be i.i.d. $\N(0, 1)$, and let $T = X/Y$. (Since $P(Y = 0) = 0$, we can define $T$ arbitrarily when $Y = 0$). Find the PDF of $T$. We can find an expression for the CDF of $T$ then differentiate: $$F_T(t) = P(T \leq t) = P\left(\frac{X}{Y} \leq t\right) = P\left(\frac{X}{|Y|} \leq t\right) = P(X \leq t|Y|).$$ We calculate $P(X \leq t|Y|)$ by integrating the joint PDF of $X$ and $Y$ where $X \leq t|Y|$ holds. We know the joint PDF is just the product of the marginal PDFS, by independence:
  \begin{align*}
    F_T(t) &= P(X \leq t|Y|) \\
           &= \int_{-\infty}^{\infty}\int_{-\infty}^{t|y|} \frac{1}{\sqrt{2\pi}}e^{-x^2/2} \frac{1}{\sqrt{2\pi}}e^{-y^2/2} \,dx \,dy \\
           &= \sqrt{\frac{2}{\pi}} \int_{0}^{\infty} e^{-y^2/2} \Phi(ty)dy.
  \end{align*}
  We differentiate this to get $$f_T(T) = F_T'(t) = \frac{1}{\pi(1 + t^2)}.$$
\end{example}
\begin{proposition}[2D LOTUS]
  Let $g : \R^2 \to \R$ be any function. If $X$ and $Y$ are discrete, then $$\E[g(X, Y)] = \sum_x \sum_y g(x, y)P(X = x, Y = y).$$ If $X$ and $Y$ are continuous, then $$\E[g(X, Y)] \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x, y) f_{X, Y}(x,y) \,dx \,dy.$$
\end{proposition}
\begin{example}[distance between two points on line]
  Consider $X, Y \simiid \t{Unif}(0, 1)$. The average distance between $X$ and $Y$ is
  \begin{align*}
    \E[|X - Y|] &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} |x - y| f_{X, Y}(x,y) \,dx \,dy \\
               &= \int_{0}^{1}\int_{0}^{1} |x - y| \,dx \,dy \\
               &= \int_{0}^{1}\int_{0}^{y} (y - x) \,dx \,dy +  \int_{0}^{1}\int_{y}^{1} (x - y) \,dx \,dy \\
               &= 2\int_{0}^{1}\int_{y}^{1} (x - y) \,dx \,dy = \frac{1}{3}. \\
  \end{align*}
  If we wanted the standard deviation of $|X - Y|$, we would need to calculate $\E[|X - Y|^2]$. This can be done by solving the double integral $$\int_{0}^{1}\int_{0}^{1} (x - y)^2 \,dx \,dy$$ or by writing $$\E[|X - Y|^2] = \E[X^2] + \E[Y^2] - 2\E[XY] = 2(\E[X^2] - \E[X^2]) = 2\Var(X) = \frac{1}{6}$$ since $\E[XY] = \E[X^2] = \E[X]\E[X]$.
\end{example}

\subsection{Covariance and Correlation}
\begin{definition}[covariance]
  The covariance between r.v.s $X$ and $Y$ is $\Cov(X, Y) = \E[(X-\E X)(Y-\E Y)]$ and measures the tendency of $X$ and $Y$ to go up or down together. Simplifying this using linearity, we get $$\Cov(X, Y) = \E[XY] - \E[X]\E[Y].$$
\end{definition}
\begin{proposition}[properties of covariance]
  Let $X$ and $Y$ be any r.v.s. Then
  \begin{enumerate}
    \item $\Cov(X, X) = \Var(X)$,
    \item $\Cov(X, Y) = \Cov(Y, X)$,
    \item $\Cov(X, c) = 0$ for any constant $c$,
    \item $\Cov(aX, Y) = a\Cov(X, Y)$ for any constant $a$,
    \item $\Cov(X + Y, Z + W) = \Cov(X, Z) + \Cov(X, W) + \Cov(Y, Z) + \Cov(Y, W)$,
    \item $\Var(X + Y) = \Var(X) \Var(Y) + 2\Cov(X, Y)$.
  \end{enumerate}
\end{proposition}
\begin{definition}[correlation]
  The correlation between r.v.s $X$ and $Y$ is $$\Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)\Var(Y)}}.$$ (Correlation is undefined in the cases $\Var(X) = 0$ or $\Var(Y) = 0$.)
\end{definition}
\begin{definition}[uncorrelated r.v.s]
  If $\Cov(X, Y) = 0$, then $\Corr(X, Y) = 0$ and $X$ and $Y$ are uncorrelated.
\end{definition}
\begin{theorem}[independence and correlation]
  If $X$ and $Y$ are independent, then they are uncorrelated.
\end{theorem}
\begin{proposition}[correlation bounds]
  For any r.v.s $X$ and $Y$, $-1 \leq \Corr(X, Y) \leq 1$.
\end{proposition}

\subsection{Multinomial Distribution}
\begin{definition}
    If $\bm{X} \sim \t{Mult}_k(n, \bm{p})$, then $$P(X_1 = n_1, \ldots, X_k = n_k) = \frac{n!}{n_1! \ldots n_k!} \cdot p_1^{n_1} \ldots p_1^{n_1}$$ for any $n_1, \ldots, n_k$ satisying $n_1 + \ldots + n_k = n$ and 0 otherwise. The multinomial distribution describes how many of $n$ objects are independently placed into each of $k$ categories where the $p_k$ is the probablity an object is placed into category $j$.
\end{definition}
\begin{proposition}[multinomial marginals]
  The marginal distributions of a multinomial variable are binomial. Namely, if $\bm{X} \sim \t{Mult}_k(n, \bm{p})$, then $X_j \sim \t{Bin}(n, p_j)$.
\end{proposition}
\begin{proposition}[multinomial lumping]
  If $\bm{X} \sim \t{Mult}_k(n, \bm{p})$, then for any distinct $i$ and $j$, $X_i + X_j \sim \t{Bin}(n, p_i + p_j)$.
\end{proposition}
\begin{proposition}[multinomial conditioning]
  If $\bm{X} \sim \t{Mult}_k(n, \bm{p})$, then $$(X_2, \ldots, X_k) | X_1 = n_1 \sim \t{Mult}_{k-1}(n - n_1,( p_2', \ldots, p_k'))$$ where $p_j' = p_j/(p_2 + \ldots + p_k)$.
\end{proposition}
\begin{example}[superhero Statwoman]
  Statwoman will have $N \sim \t{Pois}(\lambda)$ battles next year. Each battle is independently with one of three villains. The probability a battle is with Villain $j$ is $p_j$. Find the joint distribution of $X_1, X_2, X_3$, where $X_j$ is the number of battles Statwoman will have with Villain $j$. By LOTP, $$P(X_1 = x_1, X_2 = x_2, X_3 = x_3) = \sum_{n=0}^{\infty}P(X_1 = x_1, X_2 = x_2, X_3 = x_3|N = n)P(N = n).$$ Notice there is only one nonzero term in this sum where $n = x_1 + x_2 + x_3$. Also notice that for a given $n$, $(X_1, X_2, X_3) \sim \t{Mult}_3(n, (p_1, p_2, p_3))$. Hence, 
  \begin{align*}
    P(X_1 = x_1, X_2 = x_2, X_3 = x_3) &= P(X_1 = x_1, X_2 = x_2, X_3 = x_3|N = n)P(N = n), \\
                                       &= \frac{n!}{x_1! x_2! x_3!} p_1^{x_1} p_2^{x_2} p_3^{x_3} \cdot e^{-\lambda} \frac{\lambda^n}{n!}, \\
                                       &=\frac{e^{-\lambda p_1}(\lambda p_1)^{x_1}}{x_1!} \cdot \frac{e^{-\lambda p_2}(\lambda p_{2})^{x_2}}{x_2!} \cdot \frac{e^{-\lambda p_3}(\lambda p_3)^{x_3}}{x_3!},
  \end{align*}
  for all $x_1, x_2, x_3 \geq 0$ and 0 otherwise. Therefore, $X_1, X_2, X_3$ are independent with $X_j \sim \t{Pois}(\lambda p_j)$.
\end{example}

\subsection{Multivariate Normal Distribution}
\begin{definition}[multivariate normal distribution, MVN distribution]
  If a $k$-dimensional random vector $\bm{X}$ has a multivariate normal distribution, then every linear combination of its features has a normal distribution. That is, $t_1X_1 + \ldots + t_kX_k$ has a normal distribution for any constants $t_1, \ldots, t_k$.
\end{definition}
\begin{proposition}[MVN marginals]
  The marginal distributions of a MVN variable are normal. Namely, if $\bm{X}$ is MVM, then $X_j$ is normal.
\end{proposition}
\begin{proposition}[subvectors of MVN variable]
  If $(X_1, X_2, X_3)$ is MVN,then $(X_1, X_2)$ is MVN.
\end{proposition}
\begin{proposition}[concatenation of MVN variables]
  If $(X_1, \ldots, X_n)$ and $(Y_1, \ldots, Y_m)$ are MVN,then $(X_1, \ldots, X_n, Y_1, \ldots, Y_m)$ is MVM.
\end{proposition}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRANSFORMATIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Transformations}

\subsection{Change of Variables}
\begin{theorem}[change of variables in one dimension]
  Let $X$ be a continuous $r.v.$ and $Y = g(X)$, where $g$ is differentiable and strictly increasing or decreasing. Then $f_Y(y) = f_X(x) |\frac{dx}{dy}|$, where $x = g^{-1}(y)$, and $S_Y = \{ g(x) : x \in S_X \}$.
\end{theorem}
\begin{example}[log-normal PDF]
  Let $X \in \N(0, 1)$ and $Y = e^X$. Since $g(x) = e^x$ is strictly increasing, we can use the change of variables formula to find the PDF of $Y$: $$f_Y(y) = f_X(x) \left| \frac{dx}{dy} \right| = \varphi(x) \frac{1}{e^x} = \varphi(\log y) \frac{1}{y}, \quad y > 0.$$ Notice we solve for $|\frac{dx}{dy}|$ by calculating $|\frac{dy}{dx}|^{-1}$, which, as in this case, is simpler. Also notice we write everything in terms of $y$ and specify $S_Y$.
\end{example}
\begin{example}[chi-square pdf]
  Let $X \in \N(0, 1)$ and $Y = X^2$. We cannot use the change of variables formula since $g(x) = x^2$ is not injective on $S_X$, so we start from the CDF. We inspect the graph of $y=x^2$ and realize with symmetry $$F_Y(y) = P(X^2 \leq y) = P(-\sqrt{y} \leq X \leq \sqrt{y}) = \Phi(\sqrt{y}) - \Phi(-\sqrt{y}) = 2\Phi(\sqrt{y}) - 1.$$ Therefore, $$f_Y(y) = \frac{\partial}{\partial y} 2\Phi(\sqrt{y}) - 1 = 2\varphi(\sqrt{y}) \cdot \frac{1}{2}y^{-1/2} = \varphi(\sqrt{y})y^{-1/2}, \quad y > 0.$$
\end{example}
\begin{example}[lighthouse]
  A lighthouse on a shore is shining a light toward the ocean at a random angle $U \sim \t{Unif}(-\pi/2, pi/2)$ in radians. Let $X$ be the point that the light hits on a parralel line 1 mile out from the shore. Namely, $X = tan(U)$. Since $g(u) = tan(u)$ is differentiable and injective over $S_U$, we can use the change of variables formula: $$f_X(x) = f_Y(u)\frac{du}{dx} = \frac{1}{\pi} \cdot \frac{1}{1 + x^2}.$$
\end{example}
\begin{theorem}[change of variables in all dimensions]
  Let $\bm{X} \in \R^n$ be a continuous random vector and $g : A_0 \to B_0$ be an invertible function where $A_0, B_0 \subseteq \R^n$ are open, $S_{\bm{X}} \subseteq A_0$, and $S_{\bm{Y}} \subseteq B_0$. Then if $Y = g(X)$, $$f_{\bm{Y}}(\bm{y}) = f_{\bm{X}}(g^{-1}(\bm{y})) \cdot |{\left| \frac{\partial \bm{x}}{\partial \bm{y}} \right|}|, \quad \bm{y} \in B_0$$ where $|{\left| \frac{\partial \bm{x}}{\partial \bm{y}} \right|}|$ is the absolute value of the determinant of the Jacobian matrix $$\frac{\partial \bm{x}}{\partial \bm{y}} = \left(\begin{array}{cccc}
    \frac{\partial x_{1}}{\partial y_{1}} & \frac{\partial x_{1}}{\partial y_{2}} & \cdots & \frac{\partial x_{1}}{\partial y_{n}} \\
    \vdots & & & \vdots \\
    \frac{\partial x_{n}}{\partial y_{1}} & \frac{\partial x_{n}}{\partial y_{2}} & \cdots & \frac{\partial x_{n}}{\partial y_{n}}
  \end{array}\right).$$
\end{theorem}
\begin{example}[Box-Muller]
  Let $U \sim \t{Unif}(0, 2\pi)$, and let $T \sim \t{Expo}(1)$ be independent of $U$. let $X \sqrt{2T} \cos U$ and $Y = \sqrt{2T} \sin U$. Then what is the PDF of $X$ and $Y$? Viewing $(X, Y)$ as a point in the plane, $$\sqrt{X^2 + Y^2} = \sqrt{2T(\cos^2U + \sin^2U)} = \sqrt{2T}$$ is the from the origin and $U$ is the angle. Hence, $(\sqrt{2T}, U)$ expresses $(X, Y)$ in polar coordinates. Note that the transformation is invertible. The Jacobian matrix $$\frac{\partial (x, y)}{\partial (u, t)} = \left(\begin{array}{cc} -\sqrt{2t}\sin u & \frac{1}{\sqrt{2t}}\cos u \\ \sqrt{2t}\cos u  & \frac{1}{\sqrt{2t}}\sin u \end{array}\right)$$ exists, has continuous entries, and has absolute determinant $|-\sin^2 u - \cos^2 u| = 1$. Letting $x = \sqrt{2t}\cos u, y = \sqrt{2t}\sin u$ to mirror the transformation from $(U, T)$ to $(X, Y)$, we have 
  \begin{align*}
    f_{X, Y}(x, y) &=f_{U, T}(u, t) \cdot|{\left| \frac{\partial(u, t)}{\partial(x, y)}\right|}| \\
                   &=\frac{1}{2 \pi} e^{-t} \cdot 1 \\
                   &=\frac{1}{2 \pi} e^{-\frac{1}{2}\left(x^{2}+y^{2}\right)} \\
                   &=\frac{1}{\sqrt{2 \pi}} e^{-x^{2} / 2} \cdot \frac{1}{\sqrt{2 \pi}} e^{-y^{2} / 2}
  \end{align*}
  for all $x, y \in \R$. Since $f_{X, Y}$ factors into a function of $x$ and a function of $y$, $X$ and $Y$ are independent. Marginally, both $X$ and $Y$ are i.i.d. $\N(0, 1)$ r.v.s.
\end{example}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONDITIONAL EXPECTATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Conditional Expectation}

\subsection{Given an Event}
\subsection{Given an R.V.}
\subsection{Conditional Variance}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INEQUALITIES AND LIMIT THEOREMS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Inequalities and Limit Theorems}

\end{document}